{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_model_MSE_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sarah0711/CUDA-Vision-Lab/blob/main/Basic_model_MSE_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feDqIqPFAs8Q"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o8trDI8vLJX"
      },
      "source": [
        "import numpy as np\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdLNs0jIJ_5r"
      },
      "source": [
        "#Define Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf3Jhs_rdFgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a456193-93c8-4b70-83ef-396d534ee424"
      },
      "source": [
        "n_epochs = 250\n",
        "batch_size_train = 20\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "random_seed = 1\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f798f43d050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgf34TpEKEiK"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsUoMKw5c5ql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "e3abb271-90a3-4b99-c4ca-067aa3a005a4"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  dsets.MNIST('./data', train=True, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  dsets.MNIST('./data', train=False, download=True,\n",
        "                             transform=transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:05, 1807948.09it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 331782.61it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5388578.67it/s]                           \n",
            "8192it [00:00, 128219.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQwi0ksTe_9W"
      },
      "source": [
        "train_size = (len(train_loader.dataset))\n",
        "test_size = (len(test_loader.dataset))\n",
        "input_size = 784\n",
        "hidden_size = 100\n",
        "output_size = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhix-IB98mC"
      },
      "source": [
        "#Defining the softmax function and derivatiev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzL8IIhK8kjL"
      },
      "source": [
        "def softmax(z):\n",
        "    z -= np.max(z,axis=1)[:,None]\n",
        "    sm =  np.exp(z) / np.sum(np.exp(z), axis=1)[:,None]\n",
        "    return sm\n",
        "\n",
        "def softmax_grad(sm):\n",
        "  grads = np.zeros(sm.shape)\n",
        "  for i in range(grads.shape[0]):\n",
        "      for j in range(sm.shape[0]):\n",
        "        delta = 1 if i==j else 0\n",
        "        grads[i] += np.multiply(sm[i],(delta-sm[j]))\n",
        "  return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us2YjYrTKXwl"
      },
      "source": [
        "#Sigmoid activataion for hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z50ULQHzkNrR"
      },
      "source": [
        "def sigmoid(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return np.multiply(x,(1-x))\n",
        "  return 1/(1+np.exp(-x)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ps82JWGKe-x"
      },
      "source": [
        "#Mean Square Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFYfOH5TIDjG"
      },
      "source": [
        "def loss(e):\n",
        "    \"\"\"MSE loss function\"\"\"\n",
        "    return np.mean((e)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZJ-9iAMKp1J"
      },
      "source": [
        "#Define weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7lkO2KbZMoJ"
      },
      "source": [
        "np.random.seed(random_seed)\n",
        "w1 = np.random.randn(input_size,hidden_size)\n",
        "b1 = np.zeros(hidden_size)\n",
        "w2 = np.random.randn(hidden_size,output_size)\n",
        "b2 = np.zeros(output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_anL2u08KtaU"
      },
      "source": [
        "#Calculate output and compare with label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIodzEVzTMH4"
      },
      "source": [
        "def evaluation():\n",
        "  acc = 0.0\n",
        "  for batch_idx, (test_data, test_target) in enumerate(test_loader):\n",
        "      testx = test_data.reshape(batch_size_test,-1)\n",
        "      testy = test_target.numpy()\n",
        "      hidden_out = sigmoid(testx.numpy() @ w1 + b1)\n",
        "      out = softmax(hidden_out @ w2 + b2 )  \n",
        "      acc += np.mean(np.argmax(out,axis=1)==testy)\n",
        "\n",
        "  acc /= batch_idx\n",
        "  print('{0:0.09f}'.format(acc*100), \"% of test examples classified correctly.\")\n",
        "  return 1.0-acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeeljUbWK5Ji"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3s7lvbLIIFT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06647d09-3cf9-40f3-b8bf-977bce34e243"
      },
      "source": [
        "\n",
        "patience = 5\n",
        "p = 0\n",
        "lowest_e = 1.0\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  number=0\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    trainx = data.reshape(batch_size_train,-1)\n",
        "    trainy = np.zeros((batch_size_train,output_size))\n",
        "    for i in range(batch_size_train):\n",
        "      trainy[i,target[i]] = 1\n",
        "\n",
        "  #feedforward\n",
        "    hidden_out = sigmoid(trainx.numpy() @ w1 + b1)\n",
        "    out = softmax(hidden_out @ w2 + b2 )\n",
        "\n",
        "  #backpropagation errors\n",
        "    error = trainy - out  \n",
        "    delta_2 = np.multiply(error , softmax_grad(out))\n",
        "    delta_w2 =  hidden_out.T @ delta_2\n",
        "    delta_w2 *= (learning_rate /batch_size_train)\n",
        "    \n",
        "    delta_b2 = learning_rate/batch_size_train * delta_2.sum(axis=0) \n",
        "    \n",
        "    delta_1 = np.multiply((delta_2 @ w2.T),sigmoid(hidden_out,True))\n",
        "    delta_w1 =  trainx.numpy().T @ delta_1\n",
        "    delta_w1 *= (learning_rate /batch_size_train)\n",
        "\n",
        "    delta_b1 = learning_rate/batch_size_train * delta_1.sum(axis=0) \n",
        "\n",
        "    w2 -= delta_w2\n",
        "    b2 -= delta_b2\n",
        "\n",
        "    w1 -= delta_w1\n",
        "    b1 -= delta_b1\n",
        "\n",
        "  if(epoch%10==0):   \n",
        "    e = evaluation()\n",
        "\n",
        "    if e < lowest_e:\n",
        "      lowest_e = e\n",
        "      p = 0\n",
        "      b_w2 = w2\n",
        "      b_b2 = b2\n",
        "      b_w1 = w1\n",
        "      b_b1 = b1\n",
        "    else:\n",
        "      p += 1\n",
        "    if p == patience:\n",
        "      break\n",
        "    \n",
        "\n",
        "  if(epoch%1==0):   \n",
        "    # print(\"error:\",str(loss(error)))\n",
        "    print('{0:0.09f}'.format(loss(error)), \" mse of training examples.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19.366666667 % of test examples classified correctly.\n",
            "0.112289626  mse of training examples.\n",
            "0.082976764  mse of training examples.\n",
            "0.094728524  mse of training examples.\n",
            "0.071332528  mse of training examples.\n",
            "0.077693352  mse of training examples.\n",
            "0.062202156  mse of training examples.\n",
            "0.063133567  mse of training examples.\n",
            "0.081338906  mse of training examples.\n",
            "0.055626645  mse of training examples.\n",
            "0.076210516  mse of training examples.\n",
            "64.800000000 % of test examples classified correctly.\n",
            "0.044538896  mse of training examples.\n",
            "0.052850815  mse of training examples.\n",
            "0.062739435  mse of training examples.\n",
            "0.044706599  mse of training examples.\n",
            "0.027355273  mse of training examples.\n",
            "0.054464552  mse of training examples.\n",
            "0.050546201  mse of training examples.\n",
            "0.050942184  mse of training examples.\n",
            "0.057128729  mse of training examples.\n",
            "0.036706673  mse of training examples.\n",
            "80.544444444 % of test examples classified correctly.\n",
            "0.029212102  mse of training examples.\n",
            "0.036726639  mse of training examples.\n",
            "0.035540838  mse of training examples.\n",
            "0.027679636  mse of training examples.\n",
            "0.019823482  mse of training examples.\n",
            "0.028607380  mse of training examples.\n",
            "0.026257894  mse of training examples.\n",
            "0.044020882  mse of training examples.\n",
            "0.048648538  mse of training examples.\n",
            "0.035751078  mse of training examples.\n",
            "84.155555556 % of test examples classified correctly.\n",
            "0.064439134  mse of training examples.\n",
            "0.048388234  mse of training examples.\n",
            "0.030322566  mse of training examples.\n",
            "0.025488683  mse of training examples.\n",
            "0.028753579  mse of training examples.\n",
            "0.021735870  mse of training examples.\n",
            "0.049994966  mse of training examples.\n",
            "0.040764099  mse of training examples.\n",
            "0.029390835  mse of training examples.\n",
            "0.018838320  mse of training examples.\n",
            "85.722222222 % of test examples classified correctly.\n",
            "0.029166238  mse of training examples.\n",
            "0.058430402  mse of training examples.\n",
            "0.043717654  mse of training examples.\n",
            "0.017427195  mse of training examples.\n",
            "0.023055955  mse of training examples.\n",
            "0.038706391  mse of training examples.\n",
            "0.041407055  mse of training examples.\n",
            "0.041296085  mse of training examples.\n",
            "0.026122356  mse of training examples.\n",
            "0.021418342  mse of training examples.\n",
            "86.611111111 % of test examples classified correctly.\n",
            "0.023614417  mse of training examples.\n",
            "0.026145677  mse of training examples.\n",
            "0.039821789  mse of training examples.\n",
            "0.038659827  mse of training examples.\n",
            "0.030374922  mse of training examples.\n",
            "0.035814949  mse of training examples.\n",
            "0.021380981  mse of training examples.\n",
            "0.005867029  mse of training examples.\n",
            "0.026751188  mse of training examples.\n",
            "0.049129129  mse of training examples.\n",
            "87.566666667 % of test examples classified correctly.\n",
            "0.016615434  mse of training examples.\n",
            "0.012543881  mse of training examples.\n",
            "0.012264257  mse of training examples.\n",
            "0.028628234  mse of training examples.\n",
            "0.044671455  mse of training examples.\n",
            "0.016459199  mse of training examples.\n",
            "0.012241773  mse of training examples.\n",
            "0.020858720  mse of training examples.\n",
            "0.050953877  mse of training examples.\n",
            "0.038430115  mse of training examples.\n",
            "87.877777778 % of test examples classified correctly.\n",
            "0.039333709  mse of training examples.\n",
            "0.033900027  mse of training examples.\n",
            "0.013879561  mse of training examples.\n",
            "0.023299939  mse of training examples.\n",
            "0.021616028  mse of training examples.\n",
            "0.028083253  mse of training examples.\n",
            "0.028128874  mse of training examples.\n",
            "0.033466085  mse of training examples.\n",
            "0.018973914  mse of training examples.\n",
            "0.015483957  mse of training examples.\n",
            "88.155555556 % of test examples classified correctly.\n",
            "0.042512150  mse of training examples.\n",
            "0.027955278  mse of training examples.\n",
            "0.033608865  mse of training examples.\n",
            "0.018368266  mse of training examples.\n",
            "0.000798864  mse of training examples.\n",
            "0.012461168  mse of training examples.\n",
            "0.028939791  mse of training examples.\n",
            "0.022258107  mse of training examples.\n",
            "0.025852796  mse of training examples.\n",
            "0.015738567  mse of training examples.\n",
            "87.966666667 % of test examples classified correctly.\n",
            "0.036936122  mse of training examples.\n",
            "0.025870129  mse of training examples.\n",
            "0.033163254  mse of training examples.\n",
            "0.036302962  mse of training examples.\n",
            "0.028931600  mse of training examples.\n",
            "0.028710875  mse of training examples.\n",
            "0.049028041  mse of training examples.\n",
            "0.014247964  mse of training examples.\n",
            "0.023444004  mse of training examples.\n",
            "0.031642806  mse of training examples.\n",
            "88.922222222 % of test examples classified correctly.\n",
            "0.049662225  mse of training examples.\n",
            "0.006827106  mse of training examples.\n",
            "0.017464123  mse of training examples.\n",
            "0.021248460  mse of training examples.\n",
            "0.016598422  mse of training examples.\n",
            "0.038839770  mse of training examples.\n",
            "0.020858408  mse of training examples.\n",
            "0.030204775  mse of training examples.\n",
            "0.024438927  mse of training examples.\n",
            "0.057430344  mse of training examples.\n",
            "88.911111111 % of test examples classified correctly.\n",
            "0.025909336  mse of training examples.\n",
            "0.033559950  mse of training examples.\n",
            "0.016684175  mse of training examples.\n",
            "0.015871838  mse of training examples.\n",
            "0.021829951  mse of training examples.\n",
            "0.016028912  mse of training examples.\n",
            "0.024421434  mse of training examples.\n",
            "0.007592717  mse of training examples.\n",
            "0.008736180  mse of training examples.\n",
            "0.031013945  mse of training examples.\n",
            "89.133333333 % of test examples classified correctly.\n",
            "0.019663059  mse of training examples.\n",
            "0.015759504  mse of training examples.\n",
            "0.003455537  mse of training examples.\n",
            "0.043200746  mse of training examples.\n",
            "0.022103116  mse of training examples.\n",
            "0.006767337  mse of training examples.\n",
            "0.012531987  mse of training examples.\n",
            "0.024414616  mse of training examples.\n",
            "0.013601447  mse of training examples.\n",
            "0.016571210  mse of training examples.\n",
            "89.355555556 % of test examples classified correctly.\n",
            "0.015367148  mse of training examples.\n",
            "0.032141215  mse of training examples.\n",
            "0.025288404  mse of training examples.\n",
            "0.012368941  mse of training examples.\n",
            "0.013970056  mse of training examples.\n",
            "0.020028401  mse of training examples.\n",
            "0.002534031  mse of training examples.\n",
            "0.027261158  mse of training examples.\n",
            "0.031815054  mse of training examples.\n",
            "0.025528016  mse of training examples.\n",
            "89.477777778 % of test examples classified correctly.\n",
            "0.044927793  mse of training examples.\n",
            "0.021120074  mse of training examples.\n",
            "0.030270070  mse of training examples.\n",
            "0.044379710  mse of training examples.\n",
            "0.041518684  mse of training examples.\n",
            "0.048746414  mse of training examples.\n",
            "0.033320190  mse of training examples.\n",
            "0.005830722  mse of training examples.\n",
            "0.046017770  mse of training examples.\n",
            "0.011100474  mse of training examples.\n",
            "89.555555556 % of test examples classified correctly.\n",
            "0.032854975  mse of training examples.\n",
            "0.019609809  mse of training examples.\n",
            "0.023427067  mse of training examples.\n",
            "0.009915058  mse of training examples.\n",
            "0.020525995  mse of training examples.\n",
            "0.010309920  mse of training examples.\n",
            "0.023688680  mse of training examples.\n",
            "0.025490376  mse of training examples.\n",
            "0.027415543  mse of training examples.\n",
            "0.014230809  mse of training examples.\n",
            "89.366666667 % of test examples classified correctly.\n",
            "0.022253834  mse of training examples.\n",
            "0.029061204  mse of training examples.\n",
            "0.027186684  mse of training examples.\n",
            "0.035207508  mse of training examples.\n",
            "0.027414636  mse of training examples.\n",
            "0.019056327  mse of training examples.\n",
            "0.010287470  mse of training examples.\n",
            "0.019880804  mse of training examples.\n",
            "0.034744299  mse of training examples.\n",
            "0.014517084  mse of training examples.\n",
            "89.444444444 % of test examples classified correctly.\n",
            "0.025646959  mse of training examples.\n",
            "0.023614741  mse of training examples.\n",
            "0.035226293  mse of training examples.\n",
            "0.030690620  mse of training examples.\n",
            "0.014752249  mse of training examples.\n",
            "0.024041729  mse of training examples.\n",
            "0.046683280  mse of training examples.\n",
            "0.027469170  mse of training examples.\n",
            "0.014904563  mse of training examples.\n",
            "0.026014770  mse of training examples.\n",
            "89.377777778 % of test examples classified correctly.\n",
            "0.026123290  mse of training examples.\n",
            "0.011156589  mse of training examples.\n",
            "0.020815471  mse of training examples.\n",
            "0.005474796  mse of training examples.\n",
            "0.012756712  mse of training examples.\n",
            "0.049050027  mse of training examples.\n",
            "0.016315253  mse of training examples.\n",
            "0.019878305  mse of training examples.\n",
            "0.021268088  mse of training examples.\n",
            "0.023510200  mse of training examples.\n",
            "89.322222222 % of test examples classified correctly.\n",
            "0.014549549  mse of training examples.\n",
            "0.012907358  mse of training examples.\n",
            "0.013371062  mse of training examples.\n",
            "0.039736595  mse of training examples.\n",
            "0.011451759  mse of training examples.\n",
            "0.025618808  mse of training examples.\n",
            "0.038911395  mse of training examples.\n",
            "0.025992973  mse of training examples.\n",
            "0.025050646  mse of training examples.\n",
            "0.013698974  mse of training examples.\n",
            "89.422222222 % of test examples classified correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT61MAkeK9pf"
      },
      "source": [
        "#Final Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXFeONMh4KWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "29396354-46a8-41f4-e8d6-a04118ba61ad"
      },
      "source": [
        "\n",
        "w2 = b_w2\n",
        "b2 = b_b2\n",
        "w1 = b_w1\n",
        "b1 = b_b1\n",
        "\n",
        "evaluation()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "89.422222222 % of test examples classified correctly.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10577777777777753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}